{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二章：使用spaCy进行大规模数据分析\n",
    "\n",
    "在本章中，我们会用一些新技术来从大量语料中抽取特定信息。 我们会学习如何利用spaCy的数据结构来结合统计与规则模型进行文本分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据结构 (1): Vocab, Lexemes和StringStore\n",
    "\n",
    "欢迎回来！现在你已经有一些spaCy实例的实战经验了， 是时候学习一下spaCy背后到底是怎么工作的了。\n",
    "\n",
    "在这门课中，我们要看下共享词汇表以及spaCy如何处理字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 共享词汇表和字符串库 (1)\n",
    "\n",
    "- `Vocab`: 存储那些多个文档共享的数据\n",
    "- 为了节省内存使用，spaCy将所有字符串编码为**哈希值**。\n",
    "- 字符串只在`StringStore`中通过`nlp.vocab.strings`存储一次。\n",
    "- 字符串库：双向的**查询表**\n",
    "\n",
    "```python\n",
    "nlp.vocab.strings.add(\"咖啡\")\n",
    "coffee_hash = nlp.vocab.strings[\"咖啡\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "```\n",
    "\n",
    "- 哈希是不能逆求解的，所以我们要提供共享词汇表。\n",
    "\n",
    "```python\n",
    "# 如果该字符串从未出现过则会报错\n",
    "string = nlp.vocab.strings[7962530705879205333]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 安装下载spaCy和必要的库\n",
    "\n",
    "```bash\n",
    "# 确保已经安装了中文模型\n",
    "\n",
    "pip install spacy\n",
    "python -m spacy download zh_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# 读取一个流程，创建nlp实例\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "# nlp.vocab.strings.add(\"咖啡\")\n",
    "# coffee_hash = nlp.vocab.strings[\"咖啡\"]\n",
    "# coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "# # 如果该字符串从未出现过则会报错\n",
    "# string = nlp.vocab.strings[7962530705879205333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要拿到字符串的哈希值，我们要在`nlp.vocab.strings`中查找。\n",
    "\n",
    "要拿到一个哈希值的字符串形式，我们可以查询哈希值。\n",
    "\n",
    "一个`Doc`实例也可以暴露出它的词汇表和字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 7962530705879205333\n",
      "string value: 咖啡\n"
     ]
    }
   ],
   "source": [
    "# 在 nlp.vocab.strings 中查找字符串和哈希值\n",
    "\n",
    "doc = nlp(\"我爱喝咖啡。\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"咖啡\"])\n",
    "print(\"string value:\", nlp.vocab.strings[7962530705879205333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 7962530705879205333\n"
     ]
    }
   ],
   "source": [
    "# doc 也会暴露出词汇表和字符串\n",
    "\n",
    "doc = nlp(\"我爱喝咖啡。\")\n",
    "print(\"hash value:\", doc.vocab.strings[\"咖啡\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexemes: 词汇表中的元素\n",
    "\n",
    "- 一个`Lexeme`实例是词汇表中的一个元素\n",
    "\n",
    "```python\n",
    "doc = nlp(\"我爱喝咖啡。\")\n",
    "lexeme = nlp.vocab[\"咖啡\"]\n",
    "\n",
    "# 打印词汇的属性\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)\n",
    "```\n",
    "\n",
    "- 包含了一个词的\n",
    "\n",
    "  和语境无关\n",
    "\n",
    "  的信息\n",
    "\n",
    "  - 词组的文本：`lexeme.text`和`lexeme.orth`（哈希值）\n",
    "  - 词汇的属性如`lexeme.is_alpha`\n",
    "  - **并不包含**和语境相关的词性标注、依存关系和实体标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "咖啡 7962530705879205333 True\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"我爱喝咖啡。\")\n",
    "lexeme = nlp.vocab[\"咖啡\"]\n",
    "\n",
    "# 打印词汇的属性\n",
    "print(lexeme.text, lexeme.orth, lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocab, 哈希值和语素\n",
    "\n",
    "!['I'、'love'和'coffee'三个词在Doc、Vocab和StringStore中的图解](https://course.spacy.io/vocab_stringstore_zh.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习\n",
    " - 从字符串到哈希值\n",
    " - vocab（词汇表），哈希值和词素"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从字符串到哈希值\n",
    "\n",
    "##### Part 1\n",
    "\n",
    "- 在`nlp.vocab.strings`中查找字符串”猫”来得到哈希值。\n",
    "- 查找这个哈希值来返回原先的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12262475268243743508\n",
      "猫\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(\"我养了一只猫。\")\n",
    "\n",
    "# 查找词汇\"猫\"的哈希值\n",
    "cat_hash = nlp.vocab.strings[\"猫\"]\n",
    "print(cat_hash)\n",
    "\n",
    "# 查找cat_hash来得到字符串\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(cat_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part 2\n",
    "\n",
    "  - 在`nlp.vocab.strings`中查找字符串标签”PERSON”来得到哈希值。\n",
    "  - 查找这个哈希值来返回原先的字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16486493800568926464\n",
      "人物\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(\"周杰伦是一个人物。\")\n",
    "\n",
    "# 查找标签是\"人物\"的字符串的哈希值\n",
    "person_hash = nlp.vocab.strings[\"人物\"]\n",
    "print(person_hash)\n",
    "\n",
    "# 查找person_hash来拿到字符串\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vocab（词汇表），哈希值和词素\n",
    "\n",
    "> 分析这段代码抛出错误的原因：\n",
    "> ```\n",
    "> import spacy\n",
    "> \n",
    "> # 创建一个英文和德文的nlp实例\n",
    "> nlp = spacy.blank(\"en\")\n",
    "> nlp_de = spacy.blank(\"de\")\n",
    "> \n",
    "> # 获取字符串'Bowie'的ID\n",
    "> bowie_id = nlp.vocab.strings[\"Bowie\"]\n",
    "> print(bowie_id)\n",
    "> \n",
    "> # 在vocab中查找\"Bowie\"的ID\n",
    "> print(nlp_de.vocab.strings[bowie_id])\n",
    "> ```\n",
    "> \n",
    "> 原因：The string \"Bowie\" 不在德语的vocab中，所以我们不能把哈希值转换为原始的字符串。 哈希值是不能逆求原始值的。为了解决这个问题， 我们要通过处理文本或者查找字符串把词组加入到新的vocab中， 或者使用同样的vocab把哈希值变回一个字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据结构(2)：Doc、Span和Token\n",
    "\n",
    "我们已经学习了词汇表和字符串库，现在我们可以看下最重要的几个数据结构： 文档`Doc`、其视图词符`Token`以及跨度`Span`。\n",
    "\n",
    "`Doc`是spaCy的核心数据结构之一。 当我们用`nlp`实例来处理文本时`Doc`就会被自动创建， 当然我们也可以手动初始化这个类。\n",
    "\n",
    "创建`nlp`实例之后，我们就可以从`spacy.tokens`中导入`Doc`类。\n",
    "\n",
    "这个例子中我们用了三个词来创建一个doc。空格存储在一个布尔值的列表中， 代表着对应位置的词后面是否有空格。每一个词符都有这个信息，包括最后一个词符！\n",
    "\n",
    "`Doc`类有三个参数：共享的词汇表，词汇和空格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个nlp实例\n",
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# 导入Doc类\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# 用来创建doc的词汇和空格\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# 手动创建一个doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Span跨度实例(1)\n",
    "\n",
    "一个`Span`是doc的一段包含了一个或更多的词符的截取。 `Span`类有最少三个参数：对应的doc以及span本身起始和终止的索引。 注意终止索引代表的词符是不包含在这个span里面的！\n",
    "\n",
    "![Doc中的一个含有词符索引的Span实例图解](https://course.spacy.io/span_indices.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入Doc和Span类\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# 创建doc所需要的词汇和空格\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# 手动创建一个doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# 手动创建一个span\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# 创建一个带标签的span\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# 把span加入到doc.ents中\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最佳实践\n",
    "\n",
    "- `Doc`和`Span`是非常强大的类，可以存储词语和句子的参考资料和关系。\n",
    "\n",
    "  - **不到最后就不要把结果转换成字符串**\n",
    "  - **尽可能使用词符属性**，比如用`token.i`来表示词符的索引\n",
    "\n",
    "- 别忘了传入共享词汇表`vocab`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：创建一个Doc\n",
    "\n",
    "创建 doc 和 span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy is cool!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# 导入Doc类\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# 目标文本：\"spaCy is cool!\"\n",
    "words = [\"spaCy\", \"is\", \"cool\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# 用words和spaces创建一个Doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go, get started!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# 导入Doc类\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# 目标文本：\"Go, get started!\"\n",
    "words = [\"Go\", \",\", \"get\", \"started\", \"!\"]\n",
    "spaces = [False, True, True, False, False]\n",
    "\n",
    "# 使用words和spaces创建一个Doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh, really?!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# 导入Doc类\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# 目标文本：\"Oh, really?!\"\n",
    "words = [\"Oh\", \",\", \"really\", \"?\", \"!\"]\n",
    "spaces = [False, True, False, False, False]\n",
    "\n",
    "# 用words和spaces创建一个Doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：从头开始练习Docs（文档），spans（跨度）和entities（实体）\n",
    "\n",
    "在这个练习中，我们要手动创建`Doc`和`Span`实例，然后更新命名实体。 实际中spaCy在后台也就是这么做的。 一个共享的`nlp`实例已经创建好了。\n",
    "\n",
    "- 从`spacy.tokens`中导入`Doc`和`Span`。\n",
    "- 用`Doc`类使用词组和空格直接创建一个`doc`实例。\n",
    "- 用`doc`实例创建一个”David Bowie”的`Span`，赋予它`\"PERSON\"`的标签。\n",
    "- 用一个实体的列表，也就是”David Bowie” `span`，来覆盖`doc.ents`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"zh\")\n",
    "\n",
    "# 导入Doc和Span类\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"我\", \"喜欢\", \"周\", \"杰伦\"]\n",
    "spaces = [False, False, False, False]\n",
    "\n",
    "# 用words和spaces创建一个doc\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# 为doc中的\"周杰伦\"创建一个span，并赋予其\"PERSON\"的标签\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# 把这个span加入到doc的实体中\n",
    "doc.ents = [span]  # 也可以用doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "# 打印所有实体的文本和标签\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面内容很重要，在之后我们学习编码信息提取流程的时候，我们就会发现手动创建spaCy的实例并改变其中的实体会非常方便有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据结构最佳实践\n",
    "\n",
    "```py\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(\"北京是一座美丽的城市\")\n",
    "\n",
    "# 获取所有的词符和词性标注\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # 检查当前词符是否是专有名词\n",
    "    if pos == \"PROPN\":\n",
    "        # 检查下一个词符是否是动词\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found proper noun before a verb: 北京\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(\"北京是一座美丽的城市\")\n",
    "\n",
    "# 获取所有的词符和词性标注\n",
    "token_texts = [token.text for token in doc]\n",
    "pos_tags = [token.pos_ for token in doc]\n",
    "\n",
    "for index, pos in enumerate(pos_tags):\n",
    "    # 检查当前词符是否是专有名词\n",
    "    if pos == \"PROPN\":\n",
    "        # 检查下一个词符是否是动词\n",
    "        if pos_tags[index + 1] == \"VERB\":\n",
    "            result = token_texts[index]\n",
    "            print(\"Found proper noun before a verb:\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！！ 上面内容虽然是正确的，但是最好不要在最终结果输出之前就将结果转换成字符串形式。应该要避免把词符变成字符串，因为这样的话我们就不能**够读取其属性和关系了**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二部分\n",
    "\n",
    "- 用原生的词符属性而不是`token_texts`和`pos_tags`的列表来重写代码。\n",
    "- 在`doc`中遍历每一个`token`并检查其`token.pos_`属性。\n",
    "- 用`doc[token.i + 1]`来检查下一个词符及其`.pos_`属性\n",
    "- 如果找到一个处于动词前的专有名词，我们就打印其`token.text`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "找到了动词前面的一个专有名词： 北京\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "doc = nlp(\"北京是一座美丽的城市\")\n",
    "\n",
    "# 遍历所有字符\n",
    "for token in doc:\n",
    "    # 检查当前字符是否是专有名词\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # 检查下一个字符是否是动词\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"找到了动词前面的一个专有名词：\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然上面代码例子表现不错，但还是有很多改进空间。\n",
    "如果doc是由一个专有名词结尾的，`doc[token.i + 1]` 就会报错。为了保证我们的代码能适用于更多场景，我们需要首先检查下是否 `token.i + 1 < len(doc)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量和语义相似度计算（重要）\n",
    "\n",
    "本节课中我们要学习如何用spaCy来判断 文档document、跨度span或者词符token之间有多相似。\n",
    "\n",
    "我们还会学到如何在实际的自然语言处理应用中用到词向量。\n",
    "\n",
    "非常重要的一点：**要计算相似度，我们必须需要一个比较大的含有词向量的spaCy流程。**\n",
    "\n",
    "- `spaCy`可以对比两个实例来判断它们之间的相似度\n",
    "\n",
    "- `Doc.similarity()`、`Span.similarity()`和`Token.similarity()`\n",
    "\n",
    "- 使用另一个实例作为参数返回一个相似度分数(在`0`和`1`之间)\n",
    "\n",
    "- 注意：\n",
    "\n",
    "  我们需要一个含有词向量的流程，比如：\n",
    "\n",
    "  - ✅ `en_core_web_md` (中等)\n",
    "  - ✅ `en_core_web_lg` (大)\n",
    "  - 🚫 **而不是** `en_core_web_sm` (小)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.793683571249384\n"
     ]
    }
   ],
   "source": [
    "# 相似度举例\n",
    "\n",
    "# 读取一个有词向量的较大流程\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_md\")\n",
    "\n",
    "# 比较两个文档\n",
    "doc1 = nlp(\"怎么解锁宠物之家\")\n",
    "doc2 = nlp(\"怎么开启宠物的家？\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[西红柿, 和, 番茄, 是, 一样, 的]\n",
      "0.41378068923950195\n"
     ]
    }
   ],
   "source": [
    "# 比较两个词符\n",
    "doc = nlp(\"西红柿和番茄是一样的\")\n",
    "\n",
    "print([text for text in doc])\n",
    "\n",
    "token1 = doc[2]\n",
    "token2 = doc[0]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5687867812442264\n"
     ]
    }
   ],
   "source": [
    "# 对比一篇文章和一个词符\n",
    "doc = nlp(\"怎么解锁宠物之家\")\n",
    "token = nlp(\"家\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[宠物, 的, 家]\n",
      "0.7687521259814877\n"
     ]
    }
   ],
   "source": [
    "# 对比一个跨度span和一篇文档\n",
    "span = nlp(\"怎么解锁宠物的家\")[2:5]\n",
    "print([text for text in span])\n",
    "\n",
    "doc = nlp(\"如何开启宠物之家\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy是如何判断相似度的？\n",
    "\n",
    "- 相似度是通过**词向量**计算的\n",
    "- 词向量是一个词汇的多维度的语义表示\n",
    "- 词向量是用诸如[Word2Vec](https://en.wikipedia.org/wiki/Word2vec) 这样的算法在大规模语料上面生成的\n",
    "- 词向量可以是spaCy流程的一部分\n",
    "- 默认我们使用余弦相似度，但也有其它计算相似度的方法\n",
    "- `Doc`和`Span`的向量默认是由其词符向量的平均值计算得出的\n",
    "- 短语的向量表示要优于长篇文档，因为后者含有很多不相关的词\n",
    "\n",
    "spaCy在后台到底是怎么做到相似度计算的？\n",
    "\n",
    "相似度是通过词向量计算的，词向量是一个词汇的多维度的语义表示。\n",
    "\n",
    "可能你听说过Word2Vec，这就是常常被用来从原始语料中训练出词向量的其中一种算法。\n",
    "\n",
    "词向量可以是spaCy流程的一部分。\n",
    "\n",
    "默认spaCy会返回两个向量的余弦相似度，但有需要时我们也可以替换为其它计算相似度的方法。\n",
    "\n",
    "一个包含了多个词符的实例，比如Doc和Span， 默认的向量值的计算方法是其中所有词符向量的平均值。\n",
    "\n",
    "这也是为什么通常**短语**的向量更有价值因为其中不相关的词会比较少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy中的词向量\n",
    "\n",
    "这个例子能让我们大致了解这些向量长什么样。\n",
    "\n",
    "首先我们再次读取中等大小的流程，这个流程含有词向量。\n",
    "\n",
    "然后我们处理一段文本，用.vector属性来查找一个词符的向量。\n",
    "\n",
    "结果是\"banana\"这个词的一个300维的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[怎么, 快速, 升级]\n",
      "[-9.39499855e-01 -5.93200028e-01  2.31710005e+00  2.77328491e+00\n",
      " -1.34800017e-01 -8.59369993e-01 -4.55399990e+00  2.89321494e+00\n",
      "  7.53765011e+00  2.06819987e+00  1.26613998e+00 -4.04060006e-01\n",
      " -2.85529995e+00  5.92665017e-01  1.09713006e+00 -1.07256508e+00\n",
      "  2.08375001e+00 -3.00470018e+00 -2.02804494e+00 -1.18826497e+00\n",
      "  1.76103950e+00  2.19831514e+00  6.31200016e-01 -9.24480081e-01\n",
      " -1.89569998e+00  1.64084995e+00 -1.80553007e+00 -6.46590054e-01\n",
      " -4.35134983e+00 -4.70749974e-01 -1.77869999e+00  1.98154497e+00\n",
      "  1.51329494e+00 -7.25255013e-01 -2.05837989e+00 -6.46412015e-01\n",
      " -2.23334998e-01  3.01529002e+00 -4.66424990e+00 -6.59861982e-01\n",
      " -2.69689989e+00  1.92429996e+00 -2.08240008e+00 -3.80239987e+00\n",
      "  5.43229997e-01  3.21155000e+00 -1.86499953e-02 -1.31601501e+00\n",
      " -3.75600010e-01  2.30875015e+00 -1.38937998e+00 -2.38456011e+00\n",
      " -2.46749997e+00  1.16578007e+00 -1.04484999e+00 -1.93719995e+00\n",
      " -4.76029968e+00 -1.22871494e+00 -1.85000896e-03 -1.53197002e+00\n",
      "  2.26959991e+00  4.41619992e-01  1.48495996e+00  1.70970011e+00\n",
      " -2.54739523e+00 -3.22889996e+00  1.61160994e+00 -1.89041495e+00\n",
      " -1.81276011e+00  2.81585002e+00  2.44085002e+00  5.06929994e-01\n",
      " -9.83544946e-01 -1.37313497e+00  6.15399957e-01  1.27999783e-02\n",
      " -3.52270007e+00  3.19650054e-01  2.09819984e+00 -1.36803496e+00\n",
      "  1.49553001e+00 -2.25900006e+00 -2.22650003e+00  1.90840507e+00\n",
      "  1.65340006e+00 -3.05735493e+00 -1.32905006e+00  7.97214985e-01\n",
      "  4.22760010e-01 -1.65913689e+00 -1.03717995e+00  1.15954995e+00\n",
      "  4.05450010e+00 -1.12262511e+00 -1.15359998e+00 -3.22159982e+00\n",
      " -3.03702998e+00  9.23245013e-01 -3.58904982e+00  1.82689500e+00\n",
      "  2.88384974e-01  3.12899995e+00 -2.80439997e+00 -1.17221005e-01\n",
      " -2.38250494e+00  3.24000001e+00  1.84969997e+00  1.12759995e+00\n",
      "  6.11849904e-01 -9.81499970e-01  1.41892004e+00 -1.23129499e+00\n",
      " -1.40740001e+00  1.57824993e+00 -2.95485497e+00 -3.39540005e+00\n",
      "  2.35935497e+00 -2.60769987e+00  1.67794001e+00 -2.58447003e+00\n",
      " -3.06669998e+00  8.36449981e-01 -3.22730017e+00  3.60354996e+00\n",
      " -2.13553500e+00  1.79447496e+00  1.58589506e+00  2.29134989e+00\n",
      "  2.51464987e+00 -9.61531028e-02 -6.48136020e-01  2.93394983e-01\n",
      " -6.35050058e-01 -3.87825012e-01  9.89690006e-01  7.26500750e-02\n",
      " -1.93376005e+00 -4.89400029e-02  4.54299986e-01 -5.14198482e-01\n",
      "  4.40549994e+00  3.27250004e-01 -2.68694997e+00 -7.17435002e-01\n",
      "  7.57900000e-01 -2.40254998e+00  9.89549994e-01 -2.73390007e+00\n",
      "  5.30405045e+00  2.40486503e+00  2.17014998e-01  1.87245011e+00\n",
      " -3.27059984e+00 -6.74915075e-01 -4.40614969e-01 -4.57200050e-01\n",
      "  1.40314949e+00 -1.88908011e-01  1.42482495e+00 -1.33363998e+00\n",
      " -2.51970005e+00 -1.21900082e-01  2.72694993e+00  9.41300035e-01\n",
      "  2.43470001e+00  1.33104992e+00  1.84338510e-01  2.95500040e-01\n",
      " -8.96019459e-01  5.54450035e-01 -1.60739994e+00  1.14312994e+00\n",
      " -3.62409973e+00 -2.11004996e+00 -8.50070000e-01  1.51861000e+00\n",
      " -4.70705032e-01 -1.45710504e+00  4.58000302e-02  5.94740009e+00\n",
      " -1.48230004e+00 -1.49384999e+00  2.03020000e+00  4.09424973e+00\n",
      " -7.20205009e-01  2.66214991e+00  6.81994498e-01 -6.79689980e+00\n",
      "  2.16030002e+00 -2.11895013e+00 -3.66366506e+00  8.19715023e-01\n",
      "  2.72860003e+00 -1.15314496e+00 -1.82605004e+00  5.63600004e-01\n",
      " -5.92649937e-01  3.01127005e+00  5.93500018e-01  1.25711501e+00\n",
      "  5.44252515e-01  4.02250004e+00 -3.10929984e-01  3.55999994e+00\n",
      "  5.30964971e-01  2.11293006e+00  1.43653500e+00 -9.89804983e-01\n",
      " -9.39964950e-01 -1.59523499e+00 -2.41999626e-02 -1.24887002e+00\n",
      " -6.74364984e-01 -3.61000013e+00  1.23178995e+00  3.35914993e+00\n",
      " -7.15550005e-01 -2.21310496e+00  1.21291006e+00  1.44852006e+00\n",
      "  1.01952004e+00 -3.23234987e+00  3.01240015e+00  5.22509992e-01\n",
      " -2.40074992e+00 -1.77299500e+00 -1.23734996e-01 -9.90984976e-01\n",
      " -3.14055014e+00  2.98804998e+00  3.73385000e+00  8.50600004e-02\n",
      " -1.34270501e+00 -2.93200016e-01 -5.88299990e-01 -1.53171992e+00\n",
      "  2.81549990e-01  1.33819997e+00  2.49629998e+00 -2.79933006e-01\n",
      " -1.13625002e+00 -2.45029998e+00 -1.22985005e+00  1.18713498e+00\n",
      " -3.64014000e-01 -2.54170012e+00  7.70444989e-01  4.55000401e-02\n",
      " -2.69509983e+00  1.79755998e+00  2.88804007e+00 -1.29008007e+00\n",
      "  1.81688499e+00  1.96647501e+00 -1.15765005e-01  4.19885015e+00\n",
      "  2.70585012e+00  2.88124990e+00  5.56180000e-01  4.41219997e+00\n",
      "  1.35950005e+00 -8.46550047e-01 -6.16900027e-02 -1.38835001e+00\n",
      "  2.91829491e+00 -2.87479997e-01 -9.40864980e-01 -2.30404988e-01\n",
      "  1.73900008e+00 -6.84035003e-01 -6.57149971e-01  1.60469496e+00\n",
      "  1.60429990e+00  1.15710497e+00 -1.23213005e+00 -1.12690008e+00\n",
      "  1.42339993e+00  2.82205009e+00 -1.36508489e+00 -5.08149981e-01\n",
      "  2.42805004e+00 -2.91429996e-01  3.67594987e-01  1.77974987e+00\n",
      "  2.30530000e+00  9.25125003e-01  6.93444982e-02  1.98300993e+00\n",
      "  2.73414993e+00 -3.10065007e+00  5.97550035e-01  6.20329976e-01\n",
      " -1.16918993e+00 -4.13040018e+00  1.40274501e+00  6.84054971e-01\n",
      " -2.91794991e+00  3.38044977e+00  8.23054969e-01 -1.61755991e+00]\n"
     ]
    }
   ],
   "source": [
    "# 导入一个含有词向量的较大的流程\n",
    "nlp = spacy.load(\"zh_core_web_lg\")\n",
    "\n",
    "doc = nlp(\"怎么快速升级\")\n",
    "print([text for text in doc])\n",
    "\n",
    "# 通过token.vector属性获取向量\n",
    "print(doc[1:3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于应用场景的相似度\n",
    "\n",
    "- 对很多应用都很有用，比如推荐系统、查重系统等\n",
    "- “相似度”并没有一个客观的定义方法\n",
    "- 主要还是决定于实际场景和所支持的应用是要做什么\n",
    "\n",
    "```py\n",
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "\n",
    "print(doc1.similarity(doc2))\n",
    ">>>> 0.9501447503553421\n",
    "```\n",
    "\n",
    "我们看一个例子：spaCy默认的词向量会对\"I like cats\"和\"I hate cats\"这两句哈给出非常高的相似度分数。\n",
    "\n",
    "这是有道理的，因为两个文本都在讲关于猫的看法。\n",
    "\n",
    "但在另一个应用场景里，你可能希望这两句话是 *非常不同* 的，因为它们的看法是完全相反的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：检查词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "两\n",
      "只\n",
      "老虎\n",
      "跑\n",
      "得\n",
      "快\n",
      "[ 5.7924e-01 -2.6305e-01 -1.4191e-01 -5.0995e+00  3.8716e+00  3.5153e+00\n",
      " -6.9870e-01  2.6803e+00 -9.4611e-01  5.0214e+00  1.6222e+00  1.5286e+00\n",
      "  8.7571e-01  2.6110e+00  2.3262e-01  2.5249e+00 -1.6588e+00  1.8337e+00\n",
      " -2.5249e+00  3.8427e+00  2.6680e+00 -8.2123e-01  1.6126e+00 -3.7706e+00\n",
      "  3.8015e+00  7.8155e-02  1.5115e+00  2.8359e-01  3.1309e+00  1.5774e+00\n",
      " -5.5651e-01 -1.7239e+00 -3.7953e+00 -6.6034e-01 -9.2233e-01  4.9122e-01\n",
      " -1.4692e+00 -2.8478e+00 -4.9413e+00 -2.4462e+00  1.6943e+00  2.3306e+00\n",
      "  8.0750e-01  2.1319e+00  3.7470e+00  2.7392e+00  3.2851e+00  3.2695e+00\n",
      "  2.8855e+00 -1.7605e+00 -5.0122e-01 -3.4106e-01 -3.4631e+00  7.9958e-01\n",
      " -1.7841e+00  1.5095e-01 -2.5067e+00  4.1346e-01 -3.0813e-01  6.0523e-02\n",
      " -1.8828e+00  8.6008e-01 -2.3530e+00  1.5310e+00  5.9555e-01 -1.6424e+00\n",
      " -1.4507e+00  1.9775e+00 -2.0973e+00 -4.8088e-01 -6.6589e-01  1.2523e+00\n",
      "  1.5388e+00 -3.8856e+00 -1.0470e+00 -6.1471e-01 -1.2202e+00  3.0140e+00\n",
      " -2.5228e+00 -2.5406e+00 -3.6240e-01  1.1047e+00 -3.6204e-01 -1.7201e+00\n",
      " -8.3483e-01  4.8543e-01  3.7905e+00 -1.3421e+00  2.5667e+00 -3.0803e+00\n",
      "  7.0080e-01 -1.6625e+00 -2.9501e+00  3.7953e+00  1.6391e+00  1.4627e-01\n",
      " -1.5337e+00  1.0313e+00 -3.5355e-01 -3.1553e+00  3.5077e+00 -8.6935e-01\n",
      "  1.2467e+00  1.0013e+00 -2.3039e+00 -1.2872e+00 -2.9164e+00  9.3474e-01\n",
      "  1.4428e+00  4.6318e-01 -2.0986e+00 -1.4509e+00  2.2329e+00 -4.9564e+00\n",
      "  4.4020e+00 -2.2259e+00 -1.3311e+00 -2.4513e+00 -2.2716e+00 -3.0772e+00\n",
      "  4.0158e+00  1.0085e+00  1.5054e+00  3.3241e-01 -4.6241e-02  3.3969e+00\n",
      " -5.1220e-01 -3.8713e-01 -1.0150e+00  5.6705e-01  7.6756e-02 -3.7357e+00\n",
      "  9.6764e-01  7.2549e-01  1.1777e+00  5.2911e+00 -7.7748e-01 -2.3084e+00\n",
      "  2.9886e+00  5.4346e-03  2.1386e+00 -2.2922e+00  5.7575e+00 -1.6354e+00\n",
      "  6.2101e-01  1.0835e+00 -2.4092e+00 -3.2485e+00  6.4983e-01 -3.1658e+00\n",
      "  7.6660e-01  1.9486e-01  1.0793e+00 -2.8382e+00 -3.5696e+00 -1.6425e+00\n",
      " -5.3510e+00  1.6940e+00  8.6598e-01 -1.0292e+00 -3.1180e+00 -1.9218e+00\n",
      "  2.2785e+00  1.7752e+00  1.2698e-01 -2.0594e+00 -3.8164e+00  2.4909e+00\n",
      "  6.5976e-01 -1.4267e+00  1.7071e+00  1.2723e+00  8.1331e-01  1.6400e+00\n",
      "  6.0527e-01  9.4689e-01  4.1356e+00  1.4681e+00  1.4932e+00  1.0531e+00\n",
      "  1.3132e+00  2.5008e+00  6.9717e-01  8.0326e-01 -9.8515e-01  3.6686e-01\n",
      "  1.3508e+00 -8.1932e-01 -3.8115e+00  2.4958e+00 -1.3414e+00  2.5231e+00\n",
      "  1.8265e+00 -5.3526e+00  1.2001e+00 -1.3428e+00  1.7890e+00 -8.8131e-01\n",
      "  1.5124e+00 -3.9097e-01 -1.9924e+00 -2.0793e+00  3.4676e+00  4.1675e-01\n",
      " -3.0836e+00 -3.6522e-01  1.0776e-01  2.4466e-01  1.8187e+00 -1.0750e+00\n",
      " -7.6324e-01 -1.9996e-02 -2.4298e+00  9.1939e-01  7.0144e-01  2.8980e+00\n",
      " -2.0506e+00 -1.2472e+00 -2.6498e+00 -8.1173e-01 -9.3867e-01  3.8803e+00\n",
      "  8.0914e-01  1.3228e+00 -6.7176e-01  1.2085e+00 -2.2681e+00 -1.7464e+00\n",
      "  1.9539e+00 -2.0198e+00 -6.7864e-01 -5.4648e-01  2.4023e-01  1.4373e-01\n",
      " -1.1620e+00  8.7375e-01 -2.1149e+00  7.5399e-01 -1.0126e+00  1.5917e+00\n",
      "  1.4630e+00  2.1774e+00 -2.6580e+00 -1.0528e+00 -1.3980e+00  1.4976e+00\n",
      " -6.3626e-01 -2.4981e+00 -2.8563e+00  1.6434e+00 -1.0439e+00  8.2653e-01\n",
      "  1.1626e+00 -4.8317e+00  2.0123e-01 -9.5807e-02 -2.1817e+00 -1.6966e+00\n",
      "  4.2981e-02 -7.2740e-01  3.8890e+00 -3.2062e-01  2.0124e-01 -7.4209e-01\n",
      " -5.9732e+00 -4.1930e+00 -8.3220e-01 -2.7868e+00 -5.9619e-01 -2.0681e+00\n",
      "  1.1154e+00  1.0995e+00 -4.2168e+00  3.1343e+00 -4.1112e+00  1.5323e+00\n",
      " -1.0445e+00  1.5628e+00  1.1194e-01  3.8676e+00 -1.4527e-01 -1.8744e+00\n",
      " -5.7825e-01 -3.4847e-01 -2.7685e+00 -7.6714e-01  2.0325e-01  7.3041e-01\n",
      "  2.5122e-01  1.0219e+00  1.2716e+00 -1.0849e+00 -1.2975e+00 -1.3038e+00\n",
      "  1.1385e+00  2.0639e+00 -4.1764e+00  3.1660e+00  1.5245e+00  2.5270e+00]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# 读取zh_core_web_md流程\n",
    "nlp = spacy.load(\"zh_core_web_md\")\n",
    "\n",
    "# 处理文本\n",
    "doc = nlp(\"两只老虎跑得快\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text)\n",
    "\n",
    "# 获取词符\"老虎\"的向量\n",
    "laohu_vector = doc[2].vector\n",
    "print(laohu_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：对比相似度\n",
    "\n",
    "在这个练习中，使用spaCy的`similarity`方法来比较`Doc`、`Token`和`Span`实例，得到相似度分数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一部分\n",
    "\n",
    "- 使用`doc.similarity`方法来比较`doc1`和`doc2`的相似度并打印结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5488376705728557\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_md\")\n",
    "\n",
    "doc1 = nlp(\"这是一个温暖的夏日\")\n",
    "doc2 = nlp(\"外面阳光明媚\")\n",
    "\n",
    "# 获取doc1和doc2的相似度\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二部分\n",
    "\n",
    "- 使用`token.similarity`方法来比较`token1`和`token2`的相似度并打印结果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 电影\n",
      "1 和\n",
      "2 音乐\n",
      "0.32749298214912415\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_md\")\n",
    "\n",
    "doc = nlp(\"电影和音乐\")\n",
    "\n",
    "for i, token in enumerate(doc):\n",
    "    print(i, token.text)\n",
    "\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# 获取词符\"TV\"和\"books\"的相似度\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三部分\n",
    "\n",
    "- 为”不错的餐厅”/“很好的酒吧”创建跨度(span)。\n",
    "- 使用`span.similarity`来比较它们并打印结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 这是\n",
      "1 一家\n",
      "2 不错\n",
      "3 的\n",
      "4 餐厅\n",
      "5 。\n",
      "6 之后\n",
      "7 我们\n",
      "8 又\n",
      "9 去\n",
      "10 了\n",
      "11 一家\n",
      "12 很\n",
      "13 好的\n",
      "14 酒吧\n",
      "15 。\n",
      "0.6806249618530273\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_md\")\n",
    "\n",
    "doc = nlp(\"这是一家不错的餐厅。之后我们又去了一家很好的酒吧。\")\n",
    "\n",
    "for i, token in enumerate(doc):\n",
    "    print(i, token.text)\n",
    "\n",
    "# 给\"great restaurant\"和\"really nice bar\"分别创建span\n",
    "span1 = doc[2:5]\n",
    "span2 = doc[12:15]\n",
    "\n",
    "# 获取两个span的相似度\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：实际开发一些自然语言处理的应用并用到语义相似度可能需要在自己的数据上 *先训练词向量 再去改进一下相似度的算法。*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 流程和规则的结合\n",
    "\n",
    "统计模型vs规则\n",
    "\n",
    "|               | **统计模型**                           | **规则系统**                      |\n",
    "| ------------- | -------------------------------------- | --------------------------------- |\n",
    "| **应用场景**  | 需要根据例子来 *泛化* 的应用           | ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ ⠀⠀⠀⠀⠀⠀⠀ |\n",
    "| **真实范例**  | 产品名、人名、主语宾语关系             |                                   |\n",
    "| **spaCy功能** | 实体识别器、依存句法识别器、词性标注器 |     \n",
    "\n",
    "统计预测vs规则\n",
    "\n",
    "|               | **统计模型**                           | **规则系统**                         |\n",
    "| ------------- | -------------------------------------- | ------------------------------------ |\n",
    "| **使用场景**  | 需要根据例子来 *泛化* 的应用           | 有限个例子组成的字典                 |\n",
    "| **真实范例**  | 产品名、人名、主宾关系                 | 世界上的国家、城市、药品名、狗的种类 |\n",
    "| **spaCy功能** | 实体识别器、依存句法识别器、词性标注器 | 分词器, `Matcher`, `PhraseMatcher`   |\n",
    "\n",
    "回顾：基于规则的匹配\n",
    "\n",
    "```python\n",
    "# 用共享词汇表初始化\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# 模板是一个代表词符的字典组成的列表\n",
    "pattern = [{\"LEMMA\": \"love\", \"POS\": \"VERB\"}, {\"LOWER\": \"cats\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "# 运算符可以定义一个词符应该被匹配多少次\n",
    "pattern = [{\"TEXT\": \"very\", \"OP\": \"+\"}, {\"TEXT\": \"happy\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# 在doc上面调用matcher来返回一个(match_id, start, end)元组的列表\n",
    "doc = nlp(\"I love cats and I'm very very happy\")\n",
    "matches = matcher(doc)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用共享词汇表初始化\n",
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# 模板是一个代表词符的字典组成的列表\n",
    "pattern = [{\"LOWER\": \"喜欢\"}]\n",
    "matcher.add(\"LOVE_CATS\", [pattern])\n",
    "\n",
    "# 运算符可以定义一个词符应该被匹配多少次\n",
    "pattern = [{\"TEXT\": \"史瓦西\", \"OP\": \"+\"}, {\"TEXT\": \"史瓦西\"}]\n",
    "matcher.add(\"VERY_HAPPY\", [pattern])\n",
    "\n",
    "# 在doc上面调用matcher来返回一个(match_id, start, end)元组的列表\n",
    "doc = nlp(\"我喜欢史瓦西！\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计预测的加成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: 史瓦西\n",
      "Root token: 史瓦西\n",
      "Root head token: 发型\n",
      "Previous token: 喜欢 VERB\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"NPC\", [[{\"LOWER\": \"史瓦西\"}]])\n",
    "doc = nlp(\"我喜欢史瓦西的发型！\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)\n",
    "    # 获取span的根词符和根头词符\n",
    "    print(\"Root token:\", span.root.text)\n",
    "    print(\"Root head token:\", span.root.head.text)\n",
    "    # 获取前一个词符及其词性标注的POS标签\n",
    "    print(\"Previous token:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高效短语匹配\n",
    "\n",
    "- `PhraseMatcher`和普通正则表达式或者关键词搜索类似，但是可以直接读取词符！\n",
    "- 将`Doc`实例作为模板\n",
    "- 比`Matcher`更快更高效\n",
    "- 适用于大规模词表的匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span: 史瓦西\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"史瓦西\")\n",
    "matcher.add(\"NPC\", [pattern])\n",
    "doc = nlp(\"我喜欢史瓦西的发型！好酷一女熊\")\n",
    "\n",
    "# 遍历匹配结果\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # 获取匹配到的span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "短语匹配器phrase matcher可以从`spacy.matcher`中导入，和普通的matcher是一样的API。\n",
    "\n",
    "我们传进一个`Doc`实例而不是字典列表作为模板。\n",
    "\n",
    "然后我们就可以遍历文本中的匹配结果，这些结果中有匹配的ID和匹配的起始和终止索引。 同时也可以让我们可以创建一个匹配到的词符\"史瓦西\"的`Span`实例使我们可以做情景中的分析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习：模板调试1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么这个模板不能匹配到`doc`中的词符”Silicon Valley”？\n",
    "\n",
    "```python\n",
    "pattern = [{\"LOWER\": \"silicon\"}, {\"TEXT\": \" \"}, {\"LOWER\": \"valley\"}]\n",
    "```\n",
    "\n",
    "```python\n",
    "doc = nlp(\"Can Silicon Valley workers rein in big tech from within?\")\n",
    "```\n",
    "分词器不能为单空格创建词符，所以文本中的空格\" \"并没有变为词符。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习：模板调试2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个练习中的两个模板都出错了，匹配不到我们想要的结果。 你能改正它们吗？要是你卡住了，可以尝试把`doc`中的词符打印出来， 看看这些文本应该怎样被分割，然后调整你的模板保证每个字典表示一个词符。\n",
    "\n",
    "- 编辑`pattern1`使其可以正确匹配到所有的形容词后面跟着`\"笔记本\"`。\n",
    "- 编辑`pattern2`使其可以正确匹配到`\"锐龙\"`加上后面的数字 (LIKE*NUM) 和符号 (IS*ASCII) 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN2 锐龙4000U\n",
      "PATTERN2 锐龙4000H\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"荣耀将于7月16日发布新一代 MagicBook 锐龙笔记本，显然会配备7nm工艺、Zen2 架构的\"\n",
    "          \"全新锐龙4000系列，但具体采用低功耗的锐龙4000U 系列，还是高性能的锐龙4000H 系列，\"\n",
    "          \"目前还没有官方消息。今天，推特曝料大神公布了全新 MagicBook Pro 锐龙本的配置情况。\"\n",
    "         )\n",
    "\n",
    "# 创建匹配模板\n",
    "pattern1 = [{\"POS\": \"ADJ\"},{\"TEXT\": \"笔记本\"}]\n",
    "pattern2 = [{\"TEXT\": \"锐龙\"}, {\"LIKE_NUM\": True}, {\"IS_ASCII\": True}]\n",
    "\n",
    "# 初始化matcher并加入模板\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# 遍历匹配结果\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # 打印匹配到的字符串名字及匹配到的span的文本\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习：高效率的短语匹配\n",
    "\n",
    "有时候相比起写一些描述单个词符的模板，直接精确匹配字符串可能更高效。 在面对有限个种类的东西时尤其如此，比如世界上的所有国家。 \n",
    "\n",
    "我们已经有一个国家的列表，所以我们用它作为我们信息提取代码的基础。 变量`COUNTRIES`中存取了这些字符串名字的列表。\n",
    "\n",
    "- 导入`PhraseMatcher`并用含有共享`vocab`的变量`matcher`来初始化。\n",
    "- 加入短语模板并在’doc’上面调用matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[智利, 斯洛伐克]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import spacy\n",
    "\n",
    "with open(\"countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"zh\")\n",
    "doc = nlp(\"智利可能会从斯洛伐克进口货物\")\n",
    "\n",
    "# 导入PhraseMatcher并实例化\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# 创建Doc实例的模板然后加入matcher中\n",
    "# 下面的代码比这样的表达方式更快： [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# 在测试文档中调用matcher并打印结果\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 练习：提取国家和关系\n",
    "\n",
    "在上一个练习中，我们写了一段代码用spaCy的`PhraseMatcher`来寻找文本中的国家名。 我们现在用这个国家匹配器来匹配一段更长的文本，分析句法， 并用匹配到的国家名更新文档中的实体。\n",
    "\n",
    "- 对匹配结果进行遍历， 创建一个标签为`\"GPE\"`(geopolitical entity，地理政治实体)的`Span`。\n",
    "- 覆盖`doc.ents`中的实体，加入匹配到的跨度span。\n",
    "- 获取匹配到的跨度span中的根词符的头。\n",
    "- 打印出词符头和跨度span的文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "内战 --> 萨尔瓦多\n",
      "任务 --> 纳米比亚\n",
      "统治 --> 南非\n",
      "红色 --> 柬埔寨\n",
      "授权 --> 美国\n",
      "入侵 --> 伊拉克\n",
      "入侵 --> 科威特\n",
      "莫桑比克 --> 索马里\n",
      "莫桑比克 --> 海地\n",
      "南斯拉夫 --> 莫桑比克\n",
      "包括 --> 南斯拉夫\n",
      "损失 --> 美国\n",
      "行动 --> 索马里\n",
      "援助团 --> 卢旺达\n",
      "大屠杀 --> 卢旺达\n",
      "欧洲 --> 美国\n",
      "总统 --> 美国\n",
      "新加坡 --> 英国\n",
      "紧随 --> 新加坡\n",
      "撤资 --> 美国\n",
      "任务 --> 塞拉利昂\n",
      "陆战队 --> 英国\n",
      "入侵 --> 阿富汗\n",
      "入侵 --> 美国\n",
      "入侵 --> 伊拉克\n",
      "冲突 --> 苏丹\n",
      "共和国 --> 刚果\n",
      "内战 --> 叙利亚\n",
      "末期 --> 斯里兰卡\n",
      "地震 --> 海地\n",
      "[('萨尔瓦多', 'GPE'), ('纳米比亚', 'GPE'), ('南非', 'GPE'), ('柬埔寨', 'GPE'), ('美国', 'GPE'), ('伊拉克', 'GPE'), ('科威特', 'GPE'), ('索马里', 'GPE'), ('海地', 'GPE'), ('莫桑比克', 'GPE'), ('南斯拉夫', 'GPE'), ('美国', 'GPE'), ('索马里', 'GPE'), ('卢旺达', 'GPE'), ('卢旺达', 'GPE'), ('美国', 'GPE'), ('美国', 'GPE'), ('英国', 'GPE'), ('新加坡', 'GPE'), ('美国', 'GPE'), ('塞拉利昂', 'GPE'), ('英国', 'GPE'), ('阿富汗', 'GPE'), ('美国', 'GPE'), ('伊拉克', 'GPE'), ('苏丹', 'GPE'), ('刚果', 'GPE'), ('叙利亚', 'GPE'), ('斯里兰卡', 'GPE'), ('海地', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "with open(\"countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"zh_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# 创建一个doc并重置其已有的实体\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# 遍历所有的匹配结果\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # 创建一个标签为\"GPE\"的span\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # 覆盖doc.ents并添加这个span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # 获取这个span的根头词符\n",
    "    span_root_head = span.root.head\n",
    "    # 打印这个span的根头词符的文本及span的文本\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# 打印文档中的所有实体\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
